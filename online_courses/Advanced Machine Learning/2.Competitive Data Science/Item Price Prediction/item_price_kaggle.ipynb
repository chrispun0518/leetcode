{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a32aaa6c-94cd-4987-8f41-da728f8e4e82",
    "_uuid": "f68ee9a5-5593-49df-b349-2a3bcf2d5bab",
    "colab_type": "text",
    "id": "2KL3HW27Zo_N"
   },
   "source": [
    "# 0. Packages version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b0eebf4-0915-4738-805f-a91831ecc531",
    "_uuid": "0cf96d7a-74bc-43ab-b2ff-b256621aa26b",
    "colab_type": "text",
    "id": "eIWpQnJ5Zo_P"
   },
   "source": [
    "Here, we will first import all the packages we needed and list the verion of the packages.<br>\n",
    " \n",
    "numpy 1.17.2<br>\n",
    "pandas 0.24.2<br>\n",
    "sklearn 0.20.3<br>\n",
    "matplotlib 3.0.3<br>\n",
    "xgboost 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b4df18de-0c90-4a2a-bac3-a7f5ed59063b",
    "_uuid": "b4cc1d14-29ae-4541-8529-62d78054a69b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4608,
     "status": "ok",
     "timestamp": 1573139336510,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "cdoKPQdFZo_Q",
    "outputId": "437c1817-2089-46f7-b18b-cb587dfc3ae6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f70267da-f6bc-4d57-b72e-18835f31c5d2",
    "_uuid": "e6ccd4e6-1482-461e-8d1f-ddc58aef9ca9",
    "colab_type": "text",
    "id": "fQHpoo0rZo_W"
   },
   "source": [
    "# 1. Import Data And Simple Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2108e909-4c4a-4ea5-9bd9-000a20f19bf5",
    "_uuid": "4c3b03c3-8b63-44bf-909b-a847646c8d77",
    "colab_type": "text",
    "id": "Z4JHJikCZo_X"
   },
   "source": [
    "First, we import both train and test data and compare the distribution of two datasets. <br>\n",
    "The train dataset consists of 33 months.<br>\n",
    "The target is to use them to predict the sales of each shops in the 34th months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26104,
     "status": "ok",
     "timestamp": 1573139476403,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "diK-IRdQaj9z",
    "outputId": "93bd5e08-b289-44d4-a82b-c8924e94d54f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# import os \n",
    "# os.listdir('gdrive/My Drive/Item Price Prediction')\n",
    "# os.getcwd()\n",
    "# train = pd.read_csv('gdrive/My Drive/Item Price Prediction/sales_train.csv.gz')\n",
    "# test = pd.read_csv('gdrive/My Drive/Item Price Prediction/test.csv.gz')\n",
    "# shop_info = pd.read_csv('gdrive/My Drive/Item Price Prediction/shops.csv')\n",
    "# item_info = pd.read_csv('gdrive/My Drive/Item Price Prediction/items.csv')\n",
    "# item_cat_info = pd.read_csv('gdrive/My Drive/Item Price Prediction/item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aaa453a1-1dad-4b9c-b311-a2fe85e389a1",
    "_uuid": "3b8c36c8-652b-46dd-9a76-2320847e909a",
    "colab": {},
    "colab_type": "code",
    "id": "FqK4TKy-Zo_Y"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('sales_train.csv.gz')\n",
    "test = pd.read_csv('test.csv.gz')\n",
    "shop_info = pd.read_csv('shops.csv')\n",
    "item_info = pd.read_csv('items.csv')\n",
    "item_cat_info = pd.read_csv('item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1add52e3-bd54-4a15-857e-c5374d70efdb",
    "_uuid": "d1692811-b77e-4c65-bd7f-be9361d5f5e7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1573139571646,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "jkC6NaWgZo_a",
    "outputId": "6fe02dde-8bd7-4a14-e598-152160748746"
   },
   "outputs": [],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzM83NwYZo_d"
   },
   "source": [
    "First, we see that for the shop name, shop with id (0 and 57), (1 and 58) and (10,11) are similar and after some finding on the internet (since I dont know Russian) , they are in fact the same shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56xjH_MfZo_g"
   },
   "outputs": [],
   "source": [
    "shop_info.loc[0,:] = shop_info.loc[57,:]\n",
    "shop_info.loc[1,:] = shop_info.loc[58,:]\n",
    "shop_info.loc[10,:] = shop_info.loc[11,:]\n",
    "shop_info.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6B1A59_Zo_i"
   },
   "outputs": [],
   "source": [
    "train.loc[train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "train.loc[train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "train.loc[train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fef0ff8c-d389-4eb4-93e6-743b196b3856",
    "_uuid": "40cbc76f-01a4-4baa-9fef-9640f37cb055",
    "colab_type": "text",
    "id": "MGVeX193Zo_k"
   },
   "source": [
    "The first question pops up in my mind is that:<br>\n",
    "Does every shop in test set appears in the train data set? <br>\n",
    "If not, we will have to interpolate the sales by other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c557a34d-f3af-49ff-bd23-610b1206ed54",
    "_uuid": "e9c699b4-7fe9-49ba-b1bf-0c1e25f03575",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1543,
     "status": "ok",
     "timestamp": 1573139573467,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "FfeXBcPAZo_l",
    "outputId": "b681a81e-1ac8-4525-993f-103ceb5b3bbc"
   },
   "outputs": [],
   "source": [
    "train['shop_id'].value_counts().sort_index().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "316690b5-ff9c-41f0-8c53-6e9ff44a686e",
    "_uuid": "1e916240-8131-4fa9-97f6-342aad2dc04e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1235,
     "status": "ok",
     "timestamp": 1573139573468,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "hRXatgYoZo_n",
    "outputId": "372cf821-f71b-4eea-d3e9-521184e4d10a"
   },
   "outputs": [],
   "source": [
    "test['shop_id'].value_counts().sort_index().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35719994-aa27-493a-ae3e-0edfd8b12f50",
    "_uuid": "2e9e4f2a-3a8c-4d3e-8277-44089dae0214",
    "colab_type": "text",
    "id": "YdqRksk6Zo_p"
   },
   "source": [
    "We know that:<br>\n",
    "range of shop_id in train set: 2 - 59 (except 10) <br>\n",
    "range of shop_id in test set: 2 - 59 (except 10) <br>\n",
    "\n",
    "On the other hand, in reality, we know that not all shops have the all type of goods. That is, some item_id may not exist for some shops. This may be a hint for potential data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c8233f5-6866-429b-87e4-a59a616da13e",
    "_uuid": "45c60cf9-468a-4d3b-9b06-681b28df4b6c",
    "colab": {},
    "colab_type": "code",
    "id": "0484uuOYZo_q"
   },
   "outputs": [],
   "source": [
    "target_item = set(test.item_id)\n",
    "target_shop = set(test.shop_id)\n",
    "id_not_exist = {}\n",
    "for shop in target_shop:\n",
    "    id_not_exist[shop] = list(target_item - set(train[train['shop_id'] ==shop].item_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4232a375-2a5b-4828-aefd-7d6fabce2c7a",
    "_uuid": "3a617455-2ab9-4f94-8296-373cea499a21",
    "colab_type": "text",
    "id": "i41jNgbYZo_s"
   },
   "source": [
    "We Can see that there is a lot of item that did not record any sales in certain shop. So we will set the prediction for those (shop_id, item_id) pair to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6891926d-b866-4fee-b65c-87e64dc50c34",
    "_uuid": "3113c967-ad41-4efa-9b2d-bf7289956c42",
    "colab": {},
    "colab_type": "code",
    "id": "lTlhEoZOZo_t"
   },
   "outputs": [],
   "source": [
    "leakage_ans = pd.DataFrame(columns=['shop_id', 'item_id'])\n",
    "for shop in id_not_exist.keys():\n",
    "    items = id_not_exist[shop]\n",
    "    leakage_ans = leakage_ans.append(pd.DataFrame({'shop_id': [shop] * len(items), 'item_id' :items}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf040382-42f4-4e57-8d8e-662adf4588f5",
    "_uuid": "7b9d5d55-f59d-41f7-9c70-1e821ff7ec89",
    "colab_type": "text",
    "id": "GfFTJ-OnZo_v"
   },
   "source": [
    "Similarly, we may focus on those product who are in both train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "752ba356-4a5b-4d10-83d3-ac9e2052ef87",
    "_uuid": "ce60bdc1-bd4a-4d63-89e4-0c3021438fae",
    "colab": {},
    "colab_type": "code",
    "id": "1yZp__2DZo_w"
   },
   "outputs": [],
   "source": [
    "#train = train.merge(test[['shop_id', 'item_id']], left_on=['shop_id', 'item_id'], right_on=['shop_id', 'item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a09b5c0c-de98-4b3e-bc29-9e2edda0f2eb",
    "_uuid": "6e6f386f-152c-441d-a8b4-c598efab63f3",
    "colab_type": "text",
    "id": "1cUCcJrbZo_y"
   },
   "source": [
    "As the data set is a time series, we will use the 20th to 31st month to predict 32nd month as the training period. <br>\n",
    "Later, we will use 21st to 32nd month to predict 33rd month as the validation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg35XEZyZo_z"
   },
   "outputs": [],
   "source": [
    "train['month'] = (train['date_block_num'] + 1) % 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7c52bb67-5f43-4609-827a-fb3b990c61a3",
    "_uuid": "29fefc4e-bb11-4cd3-93ca-c04045e92683",
    "colab": {},
    "colab_type": "code",
    "id": "AWFXm9e-Zo_1"
   },
   "outputs": [],
   "source": [
    "y_train_split = train[train['date_block_num'] == 32]\n",
    "y_train_split = y_train_split.groupby(['shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()\n",
    "y_train_split.columns = ['shop_id', 'item_id', 'item_cnt_month']\n",
    "\n",
    "train_set = train[(train['date_block_num'] > 19) & (train['date_block_num'] < 32)]\n",
    "train_split = train_set.groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()\n",
    "train_split = train_split.merge(train_set.groupby(['date_block_num', 'shop_id', 'item_id'])['item_price'].min(),\n",
    "                                left_on=['date_block_num', 'shop_id', 'item_id'],\n",
    "                                right_index=True)\n",
    "\n",
    "train_split = train_split.merge(item_info[['item_id', 'item_category_id']], left_on='item_id', right_on='item_id')\n",
    "train_split.columns = ['date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'item_price', 'item_category_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e895a2f4-20a1-4c33-b773-745d5df20b41",
    "_uuid": "35ace461-edad-462b-90a7-e7139158236f",
    "colab_type": "text",
    "id": "pyIP682HZo_3"
   },
   "source": [
    "We now have a dataframe records the number of sales for each product in each shop per month, with the name 'item_cnt_month'. <br>\n",
    "According to the instruction of the project, the target variables of the test set are integers ranging from 0 to 20, so we will clip the item_cnt_month to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6cb26e8e-c6d8-488d-abde-a671b55ff138",
    "_uuid": "d11303f6-255f-42a7-951f-bfdfde8c5e4c",
    "colab": {},
    "colab_type": "code",
    "id": "UyGaCnARZo_3"
   },
   "outputs": [],
   "source": [
    "train_split['item_cnt_month'] = train_split['item_cnt_month'].clip(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf0fee81-b6e6-4033-966b-2a734a65e294",
    "_uuid": "ae5e3356-86fb-4f2f-b314-f1c978575a03",
    "colab_type": "text",
    "id": "q1CaBh3WZo_6"
   },
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6a6af87-dd06-4bc1-9e2d-2705c121cf1a",
    "_uuid": "0f94b2a3-ac6b-4473-9715-57758da24250",
    "colab_type": "text",
    "id": "o5Pb4Mg9Zo_7"
   },
   "source": [
    "In the previous section, we can see that each shop_id appears 5100 times in the test set. We believe that the item_id in the test set is in fact a set of number that repeatedly appears 58 times. Therefore, it seems that it is interesting to mean encode the item_cnt_day of each item_id/item_category_id and shop_id to help the prediction.\n",
    "\n",
    "Therefore, we now conduct EDA to see what feature we can generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0abd665-bcfa-4063-b789-9704d517595f",
    "_uuid": "0365d3f1-d6a7-485f-a180-86cc7eae5f18",
    "colab_type": "text",
    "id": "yYCzlQilZo_8"
   },
   "source": [
    "It seems that there is quite a strong seasonal effect for the sales, especially you can see two spikes for every december. We may include past 12 month data to capture the seasonal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "14b8a923-3a7c-424f-8d28-ea921204889a",
    "_uuid": "97d19166-228b-4a9d-bf5d-443ef9715c1b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2100,
     "status": "ok",
     "timestamp": 1573139580300,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "xU70Qi96Zo_8",
    "outputId": "3b46d7a0-e9a1-4071-9d3e-c640a33b68e9"
   },
   "outputs": [],
   "source": [
    "train_split.groupby('date_block_num')['item_cnt_month'].sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6e30d6e-b2dc-43e0-9a02-c093ef1b2e73",
    "_uuid": "f03a98ed-8045-446b-a218-1d594a716851",
    "colab_type": "text",
    "id": "vC0wtm-iZpAA"
   },
   "source": [
    "Secondly, we can see that for different categories, the total item_cnt_month is different, indicating that some categories are much more popular then other. <br>\n",
    "Moreover, the patterns for the item_cnt_month of different categories are similar across different months.<br>\n",
    "This mean that we can mean encode item_category_id by the item_cnt_month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "acb19e02-0eb8-43b3-9d7c-0125923ea365",
    "_uuid": "1eff0fd5-ff7a-415e-9478-21bc28800cb6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1659,
     "status": "ok",
     "timestamp": 1573139580301,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "0UfCNF13ZpAB",
    "outputId": "eaf70322-b087-468a-de7d-f581205b884a"
   },
   "outputs": [],
   "source": [
    "for i in range(19,30):\n",
    "    plt.plot(train_split[train_split['date_block_num'] == i].groupby(['item_category_id'])['item_cnt_month'].sum())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5d0ecb84-f54c-413e-a7e0-638da2c54e89",
    "_uuid": "2e2bd3db-ea7e-49fe-a388-1f0a13dd7f9f",
    "colab_type": "text",
    "id": "F5jMB7dnZpAD"
   },
   "source": [
    "# 3.  Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86erY5YPZpAE"
   },
   "source": [
    "The feature enginnering process is quite tedious, I will just select some of those to discuss. The full engineering process is packed into functions in Section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-XgK6jJZpAF"
   },
   "source": [
    "### 3.1 Grouping categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IQvum2zZpAG"
   },
   "source": [
    "On the other hand, I can see that the item category names have a lot of common words, for example: category_id 1 to 7 starts with 'Аксессуары' which I think it can let me group the items to bigger categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1573139580660,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "MDoObwNOZpAI",
    "outputId": "a7610d26-7fca-4e94-d7c3-cb776eb5fd11"
   },
   "outputs": [],
   "source": [
    "item_cat_info['item_category_name'][1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTNUkco8ZpAM"
   },
   "outputs": [],
   "source": [
    "item_cat_info['bigger_cat'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[0])\n",
    "item_cat_info['bigger_cat'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat'])\n",
    "\n",
    "train_split = train_split.merge(item_cat_info[['item_category_id','bigger_cat']], left_on='item_category_id', right_on='item_category_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oc7tVI4ZpAN"
   },
   "source": [
    "Similar for the suffix of the item_category_name, we see PS2, PSP etc repeat several time, we can also use those suffixies as another categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lV99Aq4yZpAO"
   },
   "source": [
    "### 3.2 Adding time series statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4080c99-a5aa-4fec-8d43-159debe7b914",
    "_uuid": "99e6f085-dad5-40ba-8ede-50804d1b16bd",
    "colab_type": "text",
    "id": "dY1ze8DfZpAP"
   },
   "source": [
    "Since we believe that if the item_price drops after a period, then the product may be outdated and not popular at all. Which will mean that the future item_cnt_day decrease. So we create features about item_price changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "925bf0a6-ae44-46be-82c4-ad01826efc12",
    "_uuid": "dfd92f2a-3e57-4677-af25-b8b228060ec1",
    "colab": {},
    "colab_type": "code",
    "id": "hn52PvQpZpAP"
   },
   "outputs": [],
   "source": [
    "#pivot table to have a series of number of sales count per month for each (item,shop) pair\n",
    "train_for_fit = train_split.pivot_table(values='item_cnt_month', index=['item_id','shop_id'], columns=['date_block_num'], aggfunc=np.mean).fillna(0)\n",
    "train_for_fit.columns = ['past_block_'+ str(12 - i) for i in range(0,12)]\n",
    "pivoted_item_count = train_for_fit.copy()\n",
    "#create 3 month moving average of the sales count \n",
    "rolling_mean3m = pivoted_item_count.rolling(3,axis=1).mean().dropna(axis=1)\n",
    "rolling_mean3m.columns = ['past_block_3m_mean_'+ str(10 - i) for i in range(0,10)]\n",
    "train_for_fit = train_for_fit.merge(rolling_mean3m, left_index=True, right_index=True)\n",
    "# create min, max, mean \n",
    "train_for_fit['max_item_cnt_month'] = pivoted_item_count.max(axis=1)\n",
    "train_for_fit['min_item_cnt_month'] = pivoted_item_count.min(axis=1)\n",
    "train_for_fit['mean_item_cnt_month'] = pivoted_item_count.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0feda3a9-7c7b-4e0b-abd9-3c8fb60da7cd",
    "_uuid": "0b6a6b68-d187-4424-a56b-2462cad36873",
    "colab_type": "text",
    "id": "9n5Xh96bZpAR"
   },
   "source": [
    "The same process is done for generate average category month sales for shop_id, item_id, item_category_id, bigger_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMQJEgSlZpAS"
   },
   "source": [
    "### 3.3 Stacking different time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XOqzLi0ZpAT"
   },
   "source": [
    "Here, we deal with the features in 20th to 31st month and use that to predict 32nd months' sales figure. <br>\n",
    "we can also append the same set of features form 19th to 30th month to predict 31st months' sales month and so on,\n",
    "so as to augment the training set and capture the effect of different calender month. This will be done using a function followed by a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d0eb01d-276b-4d9c-ba8c-3122b9b565fd",
    "_uuid": "7d895d04-7881-41c8-9180-2375a8a4e4b7",
    "colab_type": "text",
    "id": "8DGxr_MWZpAU"
   },
   "source": [
    "# 4.Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9afa94a2-52b3-48a3-8da3-c491ebb73bc1",
    "_uuid": "d307b233-b212-4f05-95d9-4ab1ca683f78",
    "colab_type": "text",
    "id": "1waB_8nBZpAU"
   },
   "source": [
    "The function belows are just a pipeline to replicate all the feature engineering above, so that we can perform the same process for train, validation and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkXhLGlmZpAV"
   },
   "outputs": [],
   "source": [
    "def data_preprocess(i, train):\n",
    "    # Generate categorical features for item categories and city\n",
    "    item_cat_info['bigger_cat1'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[0])\n",
    "    item_cat_info['bigger_cat2'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[-1])\n",
    "    item_cat_info['bigger_cat1'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat1'])\n",
    "    item_cat_info['bigger_cat2'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat2'])\n",
    "    shop_info['city'] = shop_info['shop_name'].str.split(' ').apply(lambda x:x[0])\n",
    "    shop_info['city'] = LabelEncoder().fit_transform(shop_info['city'])\n",
    "    \n",
    "    if i == 'train':\n",
    "        y_split = train[train['date_block_num'] == 32]\n",
    "        X_set = train[(train['date_block_num'] >= 20) & (train['date_block_num'] < 32)]\n",
    "        y_split = y_split.groupby(['shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()\n",
    "        y_split.columns = ['shop_id', 'item_id', 'item_cnt_month']\n",
    "    elif i == 'val' : \n",
    "        y_split = train[train['date_block_num'] == 33]\n",
    "        X_set = train[(train['date_block_num'] >= 21) & (train['date_block_num'] < 33)]\n",
    "        y_split = y_split.groupby(['shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()\n",
    "        y_split.columns = ['shop_id', 'item_id', 'item_cnt_month']\n",
    "    elif i == 'test' :\n",
    "        X_set = train[(train['date_block_num'] >= 22)]\n",
    "        y_split = None\n",
    "    elif i in range(1,20):\n",
    "        y_split = train[train['date_block_num'] == i+13]\n",
    "        X_set = train[(train['date_block_num'] > i) & (train['date_block_num'] < (i+13))]\n",
    "        y_split = y_split.groupby(['shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()\n",
    "        y_split.columns = ['shop_id', 'item_id', 'item_cnt_month']\n",
    "    else :\n",
    "        raise ValueError('i can only be train, val, test or int form 1 to 19!!!')\n",
    "    \n",
    "    # merging all information to a sigle dataframe and aggregate the item sales to monthly form \n",
    "    X_split = X_set.groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].sum().reset_index()\n",
    "    \n",
    "    X_split = X_split.merge(X_set.groupby(['date_block_num', 'shop_id', 'item_id'])['item_price'].min(),\n",
    "                                    left_on=['date_block_num', 'shop_id', 'item_id'],\n",
    "                                    right_index=True)\n",
    "\n",
    "    X_split = X_split.merge(item_info[['item_id', 'item_category_id']], left_on='item_id', right_on='item_id')\n",
    "    X_split = X_split.merge(item_cat_info[['item_category_id', 'bigger_cat1', 'bigger_cat2']],\n",
    "                            left_on='item_category_id',\n",
    "                            right_on='item_category_id')\n",
    "    X_split = X_split.merge(shop_info[['city', 'shop_id']], left_on='shop_id', right_on='shop_id')\n",
    "    \n",
    "    X_split.columns = ['date_block_num', \n",
    "                       'shop_id', \n",
    "                       'item_id', \n",
    "                       'item_cnt_month', \n",
    "                       'item_price', \n",
    "                       'item_category_id', \n",
    "                       'bigger_cat1', \n",
    "                       'bigger_cat2',\n",
    "                       'city']\n",
    "    \n",
    "    #Clip the target to 20    \n",
    "    X_split['item_cnt_month'] = X_split['item_cnt_month'].clip(0,20)\n",
    "    \n",
    "    return X_split, y_split\n",
    "\n",
    "def feature_engineering(X_split, i):\n",
    "    # pivot the table to have a series of number of sales count per month for each (item,shop) pair\n",
    "    pivoted_item_shop_count = X_split.pivot_table(values='item_cnt_month', index=['item_id','shop_id'], columns='date_block_num', aggfunc=np.mean).fillna(0)\n",
    "    item_cat_info['bigger_cat1'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[0])\n",
    "    item_cat_info['bigger_cat2'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[-1])\n",
    "    item_cat_info['bigger_cat1'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat1'])\n",
    "    item_cat_info['bigger_cat2'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat2'])\n",
    "    shop_info['city'] = shop_info['shop_name'].str.split(' ').apply(lambda x:x[0])\n",
    "    shop_info['city'] = LabelEncoder().fit_transform(shop_info['city'])\n",
    "    \n",
    "    #merge the city and categorical information to single dataframe\n",
    "    X_for_fit = pivoted_item_shop_count.copy()\n",
    "    X_for_fit.columns = ['item_cnt_past_block_'+ str(i) for i in range(0, 12)]\n",
    "    X_for_fit = X_for_fit.reset_index()\n",
    "    rolling_mean3m = X_for_fit.iloc[:,-3:].mean(axis=1)\n",
    "    rolling_mean6m = X_for_fit.iloc[:,-6:].mean(axis=1)\n",
    "    rolling_mean12m = X_for_fit.iloc[:,-12:].mean(axis=1)\n",
    "    X_for_fit['item_cnt_past_block_mean3m'] = rolling_mean3m\n",
    "    X_for_fit['item_cnt_past_block_mean6m'] = rolling_mean6m\n",
    "    X_for_fit['item_cnt_past_block_mean12m'] = rolling_mean12m\n",
    "    X_for_fit = X_for_fit.merge(item_info[['item_id','item_category_id']], left_on='item_id', right_on='item_id')\n",
    "    X_for_fit = X_for_fit.merge(item_cat_info[['bigger_cat1', 'bigger_cat2', 'item_category_id']], left_on='item_category_id', right_on='item_category_id')\n",
    "    X_for_fit = X_for_fit.merge(shop_info[['city', 'shop_id']], left_on='shop_id', right_on='shop_id')\n",
    "    \n",
    "    pivoted_item_shop_count = pivoted_item_shop_count.reset_index(drop=True)\n",
    "    \n",
    "    #create 3 month moving average of the sales count\n",
    "    # rolling_mean3m = pivoted_item_shop_count.rolling(3, axis=1).mean().dropna(axis=1)\n",
    "    # rolling_mean3m.columns = ['item_shop_past_block_3m_mean_'+ str(i) for i in range(0, 10)]\n",
    "    # rolling_mean3m = rolling_mean3m.reset_index()\n",
    "    # X_for_fit = X_for_fit.merge(rolling_mean3m, left_on=['item_id','shop_id'], right_on=['item_id','shop_id'])\n",
    "    \n",
    "    #create 6 month moving average of the sales count\n",
    "    # rolling_mean6m = pivoted_item_shop_count.rolling(6, axis=1).mean().dropna(axis=1)\n",
    "    # rolling_mean6m.columns = ['item_shop_past_block_6m_mean_'+ str(i) for i in range(0, 7)]\n",
    "    # rolling_mean6m = rolling_mean6m.reset_index()\n",
    "    # X_for_fit = X_for_fit.merge(rolling_mean6m, left_on=['item_id','shop_id'], right_on=['item_id','shop_id'])\n",
    "    \n",
    "    ##create 12 month moving average of the sales count\n",
    "    # rolling_mean12m = pivoted_item_shop_count.rolling(12, axis=1).mean().dropna(axis=1)\n",
    "    # rolling_mean12m.columns = ['item_shop_past_block_12m_mean_'+ str(i) for i in range(0, 1)]\n",
    "    # rolling_mean12m = rolling_mean12m.reset_index()\n",
    "    # X_for_fit = X_for_fit.merge(rolling_mean12m, left_on=['item_id','shop_id'], right_on=['item_id','shop_id'])\n",
    "    \n",
    "    # create min, max, mean \n",
    "\n",
    "    # X_for_fit['item_shop_max'] = pivoted_item_shop_count.max(axis=1)\n",
    "    # X_for_fit['item_shop_min'] = pivoted_item_shop_count.min(axis=1)\n",
    "    # X_for_fit['item_shop_mean'] = pivoted_item_shop_count.mean(axis=1)\n",
    "\n",
    "\n",
    "    for feature in ['shop_id', 'item_category_id', 'bigger_cat1', 'bigger_cat2', 'city', 'item_id']:\n",
    "        pivoted_count = X_split.pivot_table(values='item_cnt_month', index=feature, columns ='date_block_num', aggfunc=np.mean).fillna(0)\n",
    "        pivoted_count.columns = ['item_' + feature + '_past_block_'+ str(i) for i in range(0, 12)]\n",
    "                       \n",
    "        # rolling_mean3m = pivoted_count.rolling(3,axis=1).mean().dropna(axis=1)\n",
    "        # rolling_mean3m.columns = ['item_' + feature + '_past_block3m_mean_'+ str(i) for i in range(0, 10)]\n",
    "        # X_for_fit = X_for_fit.merge(rolling_mean3m, left_on=feature, right_index=True)\n",
    "                       \n",
    "        # rolling_mean6m = pivoted_count.rolling(6,axis=1).mean().dropna(axis=1)\n",
    "        # rolling_mean6m.columns = ['item_' + feature + '_past_block6m_mean_'+ str(i) for i in range(0, 7)]\n",
    "        # X_for_fit = X_for_fit.merge(rolling_mean6m, left_on=feature, right_index=True)\n",
    "\n",
    "        # rolling_mean12m = pivoted_count.rolling(12,axis=1).mean().dropna(axis=1)\n",
    "        # rolling_mean12m.columns = ['item_' + feature + '_past_block12m_mean_'+ str(i) for i in range(0, 1)]\n",
    "        # X_for_fit = X_for_fit.merge(rolling_mean12m, left_on=feature, right_index=True)\n",
    "\n",
    "        # X_for_fit['max_' + feature + '_cnt_month'] = X_for_fit[feature].map(pivoted_count.max(axis=1))\n",
    "        # X_for_fit['min_' + feature + '_cnt_month'] = X_for_fit[feature].map(pivoted_count.min(axis=1))\n",
    "        # X_for_fit['mean_' + feature + '_cnt_month'] = X_for_fit[feature].map(pivoted_count.mean(axis=1))\n",
    "        \n",
    "        #Keep only some recent elements and recent rolling mean\n",
    "        recent_elements = pivoted_count[[('item_' + feature + '_past_block_'+ str(i)) for i in [0, 1, 2, 5, 11]]]\n",
    "        rolling_mean3m = pivoted_count.iloc[:,-3:].mean(axis=1).rename('item_' + feature + '_past_block_mean'+ str(3))\n",
    "        rolling_mean6m = pivoted_count.iloc[:,-6:].mean(axis=1).rename('item_' + feature + '_past_block_mean'+ str(6))\n",
    "        rolling_mean12m = pivoted_count.iloc[:,-12:].mean(axis=1).rename('item_' + feature + '_past_block_mean'+ str(12))\n",
    "        X_for_fit = X_for_fit.merge(recent_elements, left_on=feature, right_index=True)\n",
    "        X_for_fit = X_for_fit.merge(rolling_mean3m, left_on=feature, right_index=True)\n",
    "        X_for_fit = X_for_fit.merge(rolling_mean6m, left_on=feature, right_index=True)\n",
    "        X_for_fit = X_for_fit.merge(rolling_mean12m, left_on=feature, right_index=True)\n",
    "\n",
    "\n",
    "    X_for_fit['calendar_month'] = i % 12 + 1\n",
    "    days_of_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    X_for_fit['num_days'] = days_of_month[ (i-1) % 12 + 1]\n",
    "    return X_for_fit\n",
    "\n",
    "def merge_before_fit(X_for_fit, y_for_fit):\n",
    "    X = X_for_fit.drop(['item_category_id'], axis=1)\n",
    "    y = y_for_fit\n",
    "    full_data = X.merge(y, left_on=['shop_id', 'item_id'], right_on=['shop_id', 'item_id'], how='left').fillna(0)\n",
    "    #X_train = full_data.drop(['shop_id', 'item_id', 'item_cnt_month', 'bigger_cat1', 'bigger_cat2', 'city'], axis=1)\n",
    "    X_train = full_data.drop(['item_cnt_month'], axis=1)\n",
    "    y_train = full_data['item_cnt_month']\n",
    "    y_train = y_train.clip(0,20)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xb-V6AOdnfo_"
   },
   "outputs": [],
   "source": [
    "usecols=['item_id',\n",
    " 'shop_id',\n",
    " 'item_cnt_past_block_0',\n",
    " 'item_cnt_past_block_1',\n",
    " 'item_cnt_past_block_2',\n",
    " 'item_cnt_past_block_5',\n",
    " 'item_cnt_past_block_11',\n",
    " 'item_cnt_past_block_mean3m',\n",
    " 'item_cnt_past_block_mean6m',\n",
    " 'item_cnt_past_block_mean12m',\n",
    " 'item_category_id',\n",
    " 'bigger_cat1',\n",
    " 'bigger_cat2',\n",
    " 'city',\n",
    " 'item_shop_id_past_block_0',\n",
    " 'item_shop_id_past_block_1',\n",
    " 'item_shop_id_past_block_2',\n",
    " 'item_shop_id_past_block_5',\n",
    " 'item_shop_id_past_block_11',\n",
    " 'item_item_category_id_past_block_0',\n",
    " 'item_item_category_id_past_block_1',\n",
    " 'item_item_category_id_past_block_2',\n",
    " 'item_item_category_id_past_block_5',\n",
    " 'item_item_category_id_past_block_11',\n",
    " 'item_city_past_block_0',\n",
    " 'item_item_id_past_block_0',\n",
    " 'item_item_id_past_block_1',\n",
    " 'item_item_id_past_block_2',\n",
    " 'item_item_id_past_block_5',\n",
    " 'item_item_id_past_block_11',\n",
    " 'calendar_month',\n",
    " 'num_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ldf9P1YzZpAX"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = data_preprocess('train', train) \n",
    "X_train = feature_engineering(X_train, 20)\n",
    "X_train = X_train[usecols]\n",
    "X_train, y_train = merge_before_fit(X_train, y_train)\n",
    "for i in range(1,20):\n",
    "    X_temp, y_temp = data_preprocess(i, train) \n",
    "    X_temp = feature_engineering(X_temp, i)\n",
    "    X_temp = X_temp[usecols]\n",
    "    X_temp, y_temp = merge_before_fit(X_temp, y_temp)\n",
    "    X_train = X_train.append(X_temp)\n",
    "    y_train = y_train.append(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9EymS9cZpAZ"
   },
   "outputs": [],
   "source": [
    "X_val, y_val = data_preprocess('val', train) \n",
    "X_val = feature_engineering(X_val, 21)\n",
    "X_val = X_val[usecols]\n",
    "X_val, y_val = merge_before_fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc5b10d2-8ed8-4e63-91b7-1e4ecb5605c9",
    "_uuid": "89f6fe91-8d3e-4537-a951-8c2c6b85c021",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1573145396563,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "AaIGglvFZpAd",
    "outputId": "769d439a-286f-4ae0-8161-acfd0b44ff08"
   },
   "outputs": [],
   "source": [
    "n_estimators_xgb = np.random.choice(np.arange(100, 1000))\n",
    "max_depth_xgb = np.random.choice(np.arange(1, 11))\n",
    "reg_lambda_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "reg_alpha_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "learning_rate_xgb = np.random.choice(np.arange(0.1, 0.5,0.01))\n",
    "colsample_bylevel_xgb = np.random.choice(np.arange(0.3,0.9,0.01))\n",
    "print(n_estimators_xgb,\n",
    "max_depth_xgb,\n",
    "reg_lambda_xgb,\n",
    "reg_alpha_xgb,\n",
    "learning_rate_xgb,\n",
    "colsample_bylevel_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02f0a3e9-acbb-45cf-a8ad-2d62153f24ac",
    "_uuid": "6cf052dd-1a3b-4d3f-bda7-ddc60362b5b7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rDETDYHfZpAf",
    "outputId": "8c42ea37-82ae-42fe-a8c6-8073cb592193"
   },
   "outputs": [],
   "source": [
    "model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                          max_depth=max_depth_xgb,\n",
    "                          learning_rate=learning_rate_xgb,\n",
    "                          reg_lambda = reg_lambda_xgb,\n",
    "                          reg_alpha = reg_alpha_xgb,\n",
    "                          #colsample_bylevel = colsample_bylevel_xgb,\n",
    "                          n_jobs=-1,\n",
    "                          random_state=9,\n",
    "                          objective='reg:squarederror')\n",
    "model1.fit(X_train, y_train)\n",
    "output1 = model1.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train, output1)))\n",
    "print(np.sqrt(mean_squared_error(y_val, model1.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hVlfPDMZpAi"
   },
   "source": [
    "### 4.1 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu0u9qbyZpAi"
   },
   "source": [
    "Select feature with importance >0 so as to reduce the size of feature and make the hyperparameter search faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "763ea572-e58e-4cbd-8ad0-e9e1ffe8f7d2",
    "_uuid": "bdc2b878-a03d-44f0-8a2f-635627809057",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1573144786569,
     "user": {
      "displayName": "lovehk sing",
      "photoUrl": "",
      "userId": "11175753879735056657"
     },
     "user_tz": -480
    },
    "id": "jzpIGcQMZpAj",
    "outputId": "64f0d22d-dbf9-435e-b514-2557a2ac0c2c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance = pd.Series(model1.feature_importances_, X_train.columns)\n",
    "important_features = list(importance[importance>0].index)\n",
    "importance.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44DoklU1ZpAl",
    "outputId": "f0a4ee36-02cf-4929-e534-f0eee64f48da"
   },
   "outputs": [],
   "source": [
    "n_estimators_xgb = 15\n",
    "max_depth_xgb = 10\n",
    "reg_lambda_xgb = 0.7\n",
    "reg_alpha_xgb = 1.92\n",
    "learning_rate_xgb = 0.46\n",
    "\n",
    "print(n_estimators_xgb,\n",
    "      max_depth_xgb,\n",
    "      reg_lambda_xgb,\n",
    "      reg_alpha_xgb,\n",
    "      learning_rate_xgb)\n",
    "\n",
    "model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                          max_depth=max_depth_xgb,\n",
    "                          learning_rate=learning_rate_xgb,\n",
    "                          reg_lambda = reg_lambda_xgb,\n",
    "                          reg_alpha = reg_alpha_xgb,\n",
    "                          colsample_bylevel=colsample_bylevel_xbg\n",
    "                          n_jobs=-1,\n",
    "                          random_state=9,\n",
    "                          objective='reg:squarederror')\n",
    "model1.fit(X_train[important_features], y_train)\n",
    "output1 = model1.predict(X_train[important_features])\n",
    "print(np.sqrt(mean_squared_error(y_train, output1)))\n",
    "print(np.sqrt(mean_squared_error(y_val, model1.predict(X_val[important_features]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3881b52b-7094-41ab-8446-190ab2fc6640",
    "_uuid": "2598ff83-5019-4a85-9063-19054ab45654",
    "colab": {},
    "colab_type": "code",
    "id": "ZUREJifBZpAn",
    "outputId": "f40190cb-bc77-4100-b03c-288cc9fd59c1"
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    n_estimators_xgb = np.random.choice(np.arange(10, 100))\n",
    "    max_depth_xgb = np.random.choice(np.arange(5, 14))\n",
    "    reg_lambda_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    reg_alpha_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    learning_rate_xgb = np.random.choice(np.arange(0.01, 0.3,0.01))\n",
    "    print(n_estimators_xgb,\n",
    "          max_depth_xgb,\n",
    "          reg_lambda_xgb,\n",
    "          reg_alpha_xgb,\n",
    "          learning_rate_xgb)\n",
    "    \n",
    "    model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                              max_depth=max_depth_xgb,\n",
    "                              reg_lambda = reg_lambda_xgb,\n",
    "                              reg_alpha = reg_alpha_xgb,\n",
    "                              learning_rate=learning_rate_xgb,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=9,\n",
    "                              objective='reg:squarederror')\n",
    "    model1.fit(X_train, y_train)\n",
    "    output1 = model1.predict(X_train)\n",
    "    print(np.sqrt(mean_squared_error(y_train, output1)))\n",
    "    print(np.sqrt(mean_squared_error(y_val, model1.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yslD9mx_ZpAu"
   },
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    # random parameter\n",
    "    print('trial: ' + str(i))\n",
    "    n_estimators_xgb = np.random.choice(np.arange(10, 100))\n",
    "    max_depth_xgb = np.random.choice(np.arange(5, 14))\n",
    "    reg_lambda_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    reg_alpha_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    learning_rate_xgb = np.random.choice(np.arange(0.1, 0.5,0.01))\n",
    "    n_estimators_rf = np.random.choice(np.arange(100, 500))\n",
    "    max_depth_rf = np.random.choice(np.arange(1,11))\n",
    "\n",
    "    print(n_estimators_xgb,\n",
    "    max_depth_xgb,\n",
    "    reg_lambda_xgb,\n",
    "    reg_alpha_xgb,\n",
    "    learning_rate_xgb,\n",
    "    n_estimators_rf,\n",
    "    max_depth_rf)\n",
    "\n",
    "    #Layer 1, XGB, Randomforest, catboost and lightgbm\n",
    "    model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                              max_depth=max_depth_xgb,\n",
    "                              learning_rate=learning_rate_xgb,\n",
    "                              reg_lambda = reg_lambda_xgb,\n",
    "                              reg_alpha = reg_alpha_xgb,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=9,\n",
    "                              objective='reg:squarederror')\n",
    "    \n",
    "    model1.fit(X_train[important_features], y_train)\n",
    "    output1 = model1.predict(X_train[important_features])\n",
    "    \n",
    "    model2 = RandomForestRegressor(n_estimators=n_estimators_rf,\n",
    "                                   max_depth=max_depth_rf,\n",
    "                                   random_state=9,\n",
    "                                   n_jobs=-1)\n",
    "    model2.fit(X_train[important_features], y_train)\n",
    "    output2 = model2.predict(X_train[important_features])\n",
    "    \n",
    "    model4 = catboost.CatBoostRegressor()\n",
    "    model4.fit(X_train[important_features],y_train)\n",
    "    output4 = model4.predict(X_train[important_features])\n",
    "    \n",
    "    model5 = LGBMRegressor()\n",
    "    model5.fit(X_train[important_features], y_train)\n",
    "    output5 = model5.predict(X_train[important_features])\n",
    "    \n",
    "    #layer2: linear regression\n",
    "    input_layer2=np.c_[output1, output2, output4, output5]\n",
    "    model6 = LinearRegression()\n",
    "    model6.fit(input_layer2, y_train)\n",
    "    print('train model r2 score: ' + str(r2_score(y_train, model6.predict(input_layer2))))\n",
    "    print('train rmse score: ' + str(np.sqrt(mean_squared_error(y_train, model6.predict(input_layer2)))))\n",
    "    \n",
    "    \n",
    "    output1 = model1.predict(X_val[important_features])\n",
    "    output2 = model2.predict(X_val[important_features])\n",
    "    output4 = model4.predict(X_val[important_features])\n",
    "    output5 = model5.predict(X_val[important_features])\n",
    "    input_layer2=np.c_[output1, output2, output4, output5]\n",
    "    \n",
    "    output6 = model6.predict(input_layer2)\n",
    "    print('val model r2 score: ' + str(r2_score(y_val, model6.predict(input_layer2))))\n",
    "    print('val rmse r2 score: ' + str(np.sqrt(mean_squared_error(y_val, model6.predict(input_layer2)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a357ca71-8321-43c9-892b-dcf4d31acea1",
    "_uuid": "0b1d19f7-2c09-4768-90f6-d837bce68053",
    "colab_type": "text",
    "id": "BfgxbgHxZpAv"
   },
   "source": [
    "## Refit the model with val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-0ukCzfZpAw"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.append(X_val)\n",
    "y_train = y_train.append(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "us0jufAiZpAy"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Layer 1, XGB, Randomforest, catboost and lightgbm\n",
    "model1 = xgb.XGBRegressor(n_estimators=50,\n",
    "                          max_depth=12,\n",
    "                          learning_rate=0.17,\n",
    "                          reg_lambda = 1.33,\n",
    "                          reg_alpha = 0.29,\n",
    "                          n_jobs=-1,\n",
    "                          random_state=9,\n",
    "                          objective='reg:squarederror')\n",
    "\n",
    "model1.fit(X_train[important_features], y_train)\n",
    "output1 = model1.predict(X_train[important_features])\n",
    "\n",
    "model2 = RandomForestRegressor(n_estimators=n_estimators_rf,\n",
    "                               max_depth=max_depth_rf,\n",
    "                               random_state=9,\n",
    "                               n_jobs=-1)\n",
    "model2.fit(X_train[important_features], y_train)\n",
    "output2 = model2.predict(X_train[important_features])\n",
    "\n",
    "model4 = catboost.CatBoostRegressor()\n",
    "model4.fit(X_train[important_features],y_train)\n",
    "output4 = model4.predict(X_train[important_features])\n",
    "\n",
    "model5 = LGBMRegressor()\n",
    "model5.fit(X_train[important_features], y_train)\n",
    "output5 = model5.predict(X_train[important_features])\n",
    "\n",
    "#layer2: linear regression\n",
    "input_layer2=np.c_[output1, output2, output4, output5]\n",
    "model6 = LinearRegression()\n",
    "model6.fit(input_layer2, y_train)\n",
    "print('train model r2 score: ' + str(r2_score(y_train, model6.predict(input_layer2))))\n",
    "print('train rmse score: ' + str(np.sqrt(mean_squared_error(y_train, model6.predict(input_layer2)))))\n",
    "\n",
    "\n",
    "output1 = model1.predict(X_val[important_features])\n",
    "output2 = model2.predict(X_val[important_features])\n",
    "output4 = model4.predict(X_val[important_features])\n",
    "output5 = model5.predict(X_val[important_features])\n",
    "input_layer2=np.c_[output1, output2, output4, output5]\n",
    "\n",
    "output6 = model6.predict(input_layer2)\n",
    "print('val model r2 score: ' + str(r2_score(y_val, model6.predict(input_layer2))))\n",
    "print('val rmse r2 score: ' + str(np.sqrt(mean_squared_error(y_val, model6.predict(input_layer2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5b3c2fe9-02ff-48c0-83dc-119121067dc7",
    "_uuid": "3c01ca69-1200-4344-ba43-69e6f960d9b4",
    "colab": {},
    "colab_type": "code",
    "id": "eyTs1I19ZpA0"
   },
   "outputs": [],
   "source": [
    "model1 = xgb.XGBRegressor(n_estimators=50,\n",
    "                      max_depth=12,\n",
    "                      reg_lambda = 1.33,\n",
    "                      reg_alpha = 0.29,\n",
    "                      learning_rate = 0.17,\n",
    "                      colsample_bylevel = 0.81,\n",
    "                      n_jobs=-1,\n",
    "                      random_state=9,\n",
    "                      objective='reg:squarederror')\n",
    "model1.fit(X_train[important_features], y_train)\n",
    "output1 = model1.predict(X_train[important_features])\n",
    "print('model r2 score: ' + str(r2_score(y_train, output1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f82f6241-36a9-43bc-bc48-6d552630f4d0",
    "_uuid": "2a17a191-0812-41e6-a54a-58a147c76f79",
    "colab_type": "text",
    "id": "dX3EVfY6ZpA2"
   },
   "source": [
    "# 5. Predicting test set and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVDX4e5dZpA3"
   },
   "outputs": [],
   "source": [
    "X_test, y_test = data_preprocess('test', train) \n",
    "X_split = feature_engineering(X_test,22)\n",
    "X_split = X_split[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat_info['bigger_cat1'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[0])\n",
    "item_cat_info['bigger_cat2'] = item_cat_info['item_category_name'].str.split('-').apply(lambda x:x[-1])\n",
    "item_cat_info['bigger_cat1'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat1'])\n",
    "item_cat_info['bigger_cat2'] = LabelEncoder().fit_transform(item_cat_info['bigger_cat2'])\n",
    "shop_info['city'] = shop_info['shop_name'].str.split(' ').apply(lambda x:x[0])\n",
    "shop_info['city'] = LabelEncoder().fit_transform(shop_info['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(shop_info, left_on=['shop_id'], right_on=['shop_id'], how='left')\n",
    "test = test.merge(item_info, left_on=['item_id'], right_on=['item_id'], how='left')\n",
    "test = test.merge(item_cat_info, left_on=['item_category_id'], right_on=['item_category_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_split['item_cnt_month'] = model1.predict(X_split.drop(['shop_id', 'item_id', 'bigger_cat1', 'bigger_cat2', 'item_category_id', 'city'], axis=1)).clip(0,20)\n",
    "test = test.merge(X_split.drop(['city',\n",
    "                                   'item_category_id',\n",
    "                                   'bigger_cat1',\n",
    "                                   'bigger_cat2',\n",
    "                                   'city'], axis=1)\n",
    "                     , left_on=['item_id', 'shop_id'], right_on=['item_id', 'shop_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['calendar_month'] = 10\n",
    "test['num_days'] = 31\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['item_cnt_month'] = model1.predict(test[usecols].drop(['city', 'item_category_id'], axis=1))\n",
    "\n",
    "result = test[['ID', 'item_cnt_month']]\n",
    "\n",
    "result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5yhw5FTZpA9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #X_split['item_cnt_month'] = model1.predict(X_split.drop(['shop_id', 'item_id', 'bigger_cat1', 'bigger_cat2', 'item_category_id', 'city'], axis=1)).clip(0,20)\n",
    "\n",
    "# X_split['item_cnt_month'] = model1.predict(X_split.drop(['city', 'item_category_id'], axis=1).clip(0,20))\n",
    "\n",
    "# result = test.merge(X_split[['item_id', 'shop_id', 'item_cnt_month']], left_on=['item_id', 'shop_id'], right_on=['item_id', 'shop_id'], how='left')\n",
    "\n",
    "# result = result[['ID', 'item_cnt_month']].fillna(0)\n",
    "\n",
    "# result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAQWmUrSZpA_"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eurUdRS_ZpBB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "222605a9-fb96-48de-a0e5-ef208bd6c9e2",
    "_uuid": "f9c87d38-d1a1-433f-8e01-be04b3d7ea18",
    "colab": {},
    "colab_type": "code",
    "id": "OoWkEkdPZpBC"
   },
   "outputs": [],
   "source": [
    "X = test_for_fit.drop(['item_category_id'], axis=1)\n",
    "y= test\n",
    "full_data = X.merge(y, left_on=['shop_id', 'item_id'], right_on=['shop_id', 'item_id'])\n",
    "X_test = full_data.drop(['shop_id', 'item_id', 'ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9da8e214-ba4b-4d3b-8064-8cb8c05a0d20",
    "_uuid": "b6a2d15d-dbeb-4b40-abc8-515fe6fc107d",
    "colab": {},
    "colab_type": "code",
    "id": "gBbAzg8cZpBE"
   },
   "outputs": [],
   "source": [
    "output1 = model1.predict(X_test)\n",
    "output2 = model2.predict(X_test)\n",
    "input_layer2=np.c_[output1,output2]\n",
    "output3 = model3.predict(input_layer2)\n",
    "full_data['item_cnt_month'] = output3\n",
    "full_data['item_cnt_month'] = full_data['item_cnt_month'].clip(0,20)\n",
    "result = test.merge(full_data[['ID','item_cnt_month']], left_on='ID', right_on='ID', how='outer').fillna(0)\n",
    "result = result[['ID','item_cnt_month']]\n",
    "result['item_cnt_month'] = result['item_cnt_month'].round()\n",
    "result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVYMwjcgZpBI"
   },
   "outputs": [],
   "source": [
    "for i in range(1,20):\n",
    "    print('trial: ' + str(i))\n",
    "    n_estimators_xgb = np.random.choice(np.arange(10, 100))\n",
    "    max_depth_xgb = np.random.choice(np.arange(1, 5))\n",
    "    reg_lambda_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    reg_alpha_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    learning_rate_xgb = np.random.choice(np.arange(0.1, 0.5,0.01))\n",
    "    n_estimators_rf = np.random.choice(np.arange(100, 500))\n",
    "    max_depth_rf = np.random.choice(np.arange(1,11))\n",
    "\n",
    "    print(n_estimators_xgb,\n",
    "    max_depth_xgb,\n",
    "    reg_lambda_xgb,\n",
    "    reg_alpha_xgb,\n",
    "    learning_rate_xgb,\n",
    "    n_estimators_rf,\n",
    "    max_depth_rf)\n",
    "\n",
    "    model = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                              max_depth=max_depth_xgb,\n",
    "                              learning_rate=learning_rate_xgb,\n",
    "                              reg_lambda = reg_lambda_xgb,\n",
    "                              reg_alpha = reg_alpha_xgb,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=9,\n",
    "                              objective='reg:squarederror')\n",
    "    \n",
    "\n",
    "    X_split, y_split = data_preprocess('train', train)\n",
    "    X_split = feature_engineering(X_split)\n",
    "    X_train, y_train = merge_before_fit(X_split, y_split)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(r2_score(y_train, model.predict(X_train)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Daaj1MGEZpBJ"
   },
   "outputs": [],
   "source": [
    "34 8 1.56 0.56 0.09999999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MvP2lK2ZpBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKsjj6EfZpBP"
   },
   "outputs": [],
   "source": [
    "X_train_split, y_train_split = data_preprocess('train', train)\n",
    "X_train_split = feature_engineering(X_train_split)\n",
    "X_train, y_train = merge_before_fit(X_train_split, y_train_split)\n",
    "X_val_split, y_val_split = data_preprocess('val', train)\n",
    "X_val_split = feature_engineering(X_val_split)\n",
    "X_val, y_val = merge_before_fit(X_val_split, y_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5yZK7DmZpBR"
   },
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    print('trial: ' + str(i))\n",
    "    n_estimators_xgb = np.random.choice(np.arange(10, 100))\n",
    "    max_depth_xgb = np.random.choice(np.arange(1, 5))\n",
    "    reg_lambda_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    reg_alpha_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    learning_rate_xgb = np.random.choice(np.arange(0.1, 0.5,0.01))\n",
    "    n_estimators_rf = np.random.choice(np.arange(100, 500))\n",
    "    max_depth_rf = np.random.choice(np.arange(1,11))\n",
    "\n",
    "    print(n_estimators_xgb,\n",
    "    max_depth_xgb,\n",
    "    reg_lambda_xgb,\n",
    "    reg_alpha_xgb,\n",
    "    learning_rate_xgb,\n",
    "    n_estimators_rf,\n",
    "    max_depth_rf)\n",
    "    \n",
    "    \n",
    "    model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                              max_depth=max_depth_xgb,\n",
    "                              learning_rate=learning_rate_xgb,\n",
    "                              reg_lambda = reg_lambda_xgb,\n",
    "                              reg_alpha = reg_alpha_xgb,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=9,\n",
    "                              objective='reg:squarederror')\n",
    "    \n",
    "    model1.fit(X_train, y_train)\n",
    "    print(r2_score(y_train, model1.predict(X_train)))\n",
    "    print(r2_score(y_val, model1.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0621b5e-ab36-46c2-ac4f-e59d0e716ae6",
    "_uuid": "1019e564-972b-4abc-8e7a-338ec4c756b8",
    "colab": {},
    "colab_type": "code",
    "id": "504TCZ_aZpBS"
   },
   "outputs": [],
   "source": [
    "X_split, y_split = data_preprocess('train', train)\n",
    "X_split = feature_engineering(X_split)\n",
    "X_train, y_train = merge_before_fit(X_split, y_split)\n",
    "\n",
    "X_split, y_split = data_preprocess('val', train)\n",
    "X_split = feature_engineering(X_split)\n",
    "X_val, y_val = merge_before_fit(X_split, y_split)\n",
    "\n",
    "for i in range(1,20):\n",
    "    print('trial: ' + str(i))\n",
    "    n_estimators_xgb = np.random.choice(np.arange(10, 100))\n",
    "    max_depth_xgb = np.random.choice(np.arange(1, 5))\n",
    "    reg_lambda_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    reg_alpha_xgb = np.random.choice(np.arange(0, 2, 0.01))\n",
    "    learning_rate_xgb = np.random.choice(np.arange(0.1, 0.5,0.01))\n",
    "    n_estimators_rf = np.random.choice(np.arange(100, 500))\n",
    "    max_depth_rf = np.random.choice(np.arange(1,11))\n",
    "\n",
    "    print(n_estimators_xgb,\n",
    "    max_depth_xgb,\n",
    "    reg_lambda_xgb,\n",
    "    reg_alpha_xgb,\n",
    "    learning_rate_xgb,\n",
    "    n_estimators_rf,\n",
    "    max_depth_rf)\n",
    "\n",
    "    model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                              max_depth=max_depth_xgb,\n",
    "                              learning_rate=learning_rate_xgb,\n",
    "                              reg_lambda = reg_lambda_xgb,\n",
    "                              reg_alpha = reg_alpha_xgb,\n",
    "                              n_jobs=-1,\n",
    "                              random_state=9,\n",
    "                              objective='reg:squarederror')\n",
    "    \n",
    "    model1.fit(X_train, y_train)\n",
    "    output1 = model1.predict(X_train)\n",
    "    \n",
    "    model2 = RandomForestRegressor(n_estimators=n_estimators_rf,\n",
    "                                   max_depth=max_depth_rf,\n",
    "                                   random_state=9,\n",
    "                                   n_jobs=-1)\n",
    "    model2.fit(X_train, y_train)\n",
    "    output2 = model2.predict(X_train)\n",
    "    \n",
    "    model4 = catboost.CatBoostRegressor()\n",
    "    model4.fit(X_train,y_train)\n",
    "    output4 = model4.predict(X_train)\n",
    "    \n",
    "    model5 = LGBMRegressor()\n",
    "    model5.fit(X_train, y_train)\n",
    "    output5 = model5.predict(X_train)\n",
    "    \n",
    "    input_layer2=np.c_[output1, output2, output4, output5]\n",
    "    model6 = LinearRegression()\n",
    "    model6.fit(input_layer2, y_train)\n",
    "    print('train model r2 score: ' + str(r2_score(y_train, model6.predict(input_layer2))))\n",
    "    print('train mse score: ' + str(np.sqrt(mean_squared_error(y_train, model6.predict(input_layer2)))))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    output1 = model1.predict(X_val)\n",
    "    output2 = model2.predict(X_val)\n",
    "    output4 = model4.predict(X_val)\n",
    "    output5 = model5.predict(X_val)\n",
    "    input_layer2=np.c_[output1, output2, output4, output5]\n",
    "    \n",
    "    output6 = model6.predict(input_layer2)\n",
    "    print('val model r2 score: ' + str(r2_score(y_val, model6.predict(input_layer2))))\n",
    "    print('val mse r2 score: ' + str(np.sqrt(mean_squared_error(y_val, model6.predict(input_layer2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "146c158f-de6c-4de0-9b82-92fd0c963c89",
    "_uuid": "bcf53ff7-60e8-43c9-9faf-f8fb95f61924",
    "colab": {},
    "colab_type": "code",
    "id": "ZOmq2KQxZpBU"
   },
   "outputs": [],
   "source": [
    "X = test_for_fit\n",
    "y = test\n",
    "full_data = X.merge(y, left_on=['shop_id', 'item_id'], right_on=['shop_id', 'item_id'])\n",
    "X_test = full_data.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fcf6f81c-70d2-4696-acd4-1e2deb337d7d",
    "_uuid": "71a15f6c-deb3-43c3-8351-b646d7542a8b",
    "colab": {},
    "colab_type": "code",
    "id": "9KGufMPpZpBZ"
   },
   "outputs": [],
   "source": [
    "full_data['item_cnt_month'] = model4.predict(X_test)\n",
    "full_data['item_cnt_month'] = full_data['item_cnt_month'].clip(0,20)\n",
    "result = test.merge(full_data[['ID','item_cnt_month']], left_on='ID', right_on='ID', how='outer').fillna(0)\n",
    "result = result[['ID','item_cnt_month']]\n",
    "result['item_cnt_month'] = result['item_cnt_month'].round()\n",
    "result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UH8gp1JsZpBb"
   },
   "source": [
    "# 6. Result output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlWnt2bkZpBb"
   },
   "outputs": [],
   "source": [
    "n_estimators_xgb = 95\n",
    "max_depth_xgb = 4\n",
    "reg_lambda_xgb = 0.06\n",
    "reg_alpha_xgb = 1.45\n",
    "learning_rate_xgb = 0.29\n",
    "n_estimators_rf = 134\n",
    "max_depth_rf = 4\n",
    "\n",
    "print(n_estimators_xgb,\n",
    "max_depth_xgb,\n",
    "reg_lambda_xgb,\n",
    "reg_alpha_xgb,\n",
    "learning_rate_xgb,\n",
    "n_estimators_rf,\n",
    "max_depth_rf)\n",
    "\n",
    "X_split, y_split = data_preprocess('val', train)\n",
    "X_split = feature_engineering(X_split)\n",
    "X_val, y_val = merge_before_fit(X_split, y_split)\n",
    "\n",
    "model1 = xgb.XGBRegressor(n_estimators=n_estimators_xgb,\n",
    "                          max_depth=max_depth_xgb,\n",
    "                          learning_rate=learning_rate_xgb,\n",
    "                          reg_lambda = reg_lambda_xgb,\n",
    "                          reg_alpha = reg_alpha_xgb,\n",
    "                          n_jobs=-1,\n",
    "                          random_state=9,\n",
    "                          objective='reg:squarederror')\n",
    "\n",
    "model1.fit(X_val, y_val)\n",
    "output1 = model1.predict(X_val)\n",
    "\n",
    "model2 = RandomForestRegressor(n_estimators=n_estimators_rf,\n",
    "                               max_depth=max_depth_rf,\n",
    "                               random_state=9,\n",
    "                               n_jobs=-1)\n",
    "model2.fit(X_val, y_val)\n",
    "output2 = model2.predict(X_val)\n",
    "\n",
    "model4 = catboost.CatBoostRegressor(iterations=121)\n",
    "model4.fit(X_val,y_val)\n",
    "output4 = model4.predict(X_val)\n",
    "\n",
    "model5 = LGBMRegressor()\n",
    "model5.fit(X_val, y_val)\n",
    "output5 = model5.predict(X_val)\n",
    "\n",
    "input_layer2=np.c_[output1, output2, output4, output5]\n",
    "model6 = LinearRegression()\n",
    "model6.fit(input_layer2, y_val)\n",
    "print('train model r2 score: ' + str(r2_score(y_val, model6.predict(input_layer2))))\n",
    "print('train mse score: ' + str(np.sqrt(mean_squared_error(y_val, model6.predict(input_layer2)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKS64QRfZpBc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_split, y_split = data_preprocess('test', train)\n",
    "X_split = feature_engineering(X_split)\n",
    "X_split = test.merge(X_split, left_on=['item_id', 'shop_id'], right_on=['item_id', 'shop_id'])\n",
    "X_test = X_split.drop(['shop_id', 'item_id', 'item_category_id', 'ID', 'bigger_cat1', 'bigger_cat2', 'city'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn9tDu9zZpBd"
   },
   "outputs": [],
   "source": [
    "output1 = model1.predict(X_test)\n",
    "output2 = model2.predict(X_test)\n",
    "output4 = model4.predict(X_test)\n",
    "output5 = model5.predict(X_test)\n",
    "input_layer2=np.c_[output1, output2, output4, output5]\n",
    "\n",
    "output6 = model6.predict(input_layer2)\n",
    "X_split['item_cnt_month'] = output6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIopUcM1ZpBf"
   },
   "outputs": [],
   "source": [
    "79 10 0.19 1.17 0.4199999999999998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZSKX1XwZpBi"
   },
   "outputs": [],
   "source": [
    "xgb.XGBClassifier(max_depth=2, 1.03 1.32 0.22999999999999995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NphOA_GiZpBj"
   },
   "outputs": [],
   "source": [
    "X_split['item_cnt_month'] = model1.predict(X_test)\n",
    "\n",
    "result = test.merge(X_split[['item_id', 'shop_id', 'item_cnt_month']], left_on=['item_id', 'shop_id'], right_on=['item_id', 'shop_id'], how='left')\n",
    "\n",
    "result = result[['ID', 'item_cnt_month']].fillna(0)\n",
    "\n",
    "result.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UH8gp1JsZpBb"
   ],
   "name": "item_price_kaggle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
